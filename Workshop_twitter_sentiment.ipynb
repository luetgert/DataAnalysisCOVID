{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Twitter data\n",
    "\n",
    "\n",
    "### Date: Feb 3 2021\n",
    "\n",
    "#### Description\n",
    "\n",
    "This notebook was provided by Carina Albrecht and edited to explore a Twitter dataset containing Tweets sent during the Capitol Hill riot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data exploration\n",
    "Before we can begin exploring the Twitter data, we will install a few libraries designed to work with text. You will only need to pip install these libraries once. After the install, it is recommended to comment out these install lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textblob is a basic API for common natural language processing (NLP) tasks \n",
    "# such as part-of-speech tagging, noun phrase extraction, sentiment analysis, \n",
    "# classification, translation, and more. Learn more here https://textblob.readthedocs.io/en/dev/\n",
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The wordcloud package will help us to make wordclouds to better display keywords in the discourse\n",
    "# You can read more on wordcloud here https://pypi.org/project/wordcloud/\n",
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langdetect is a language detection library ported from Google.\n",
    "#more information is available here https://pypi.org/project/langdetect/\n",
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code installs libraries that we will need to import below for our analysis of the Twitter data. We will collect tweets during the Capitol Hill Riots and use these libraries to assist in our analysis. Remember, we must first import the libraries we need before we can begin analyzing or plotting. We are importing more libraries than just what was installed above. We will also import pandas (to work with data in a data frame), numpy (to perform basic matrix transformations), re (to work with regular expressions for strings), string (for common string operations such as concatenation, pil (for python images), nltk (for sentiment analysis), sklearn (for feature extraction and ml), and matplotlib as well as plotly (for visualizations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "import re \n",
    "from textblob import TextBlob\n",
    "import string\n",
    "\n",
    "# Word cloud visualization\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "# Machine learning (sentiment analysis)\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This one will be used to help us with lexicon\n",
    "import nltk\n",
    "\n",
    "# Other visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"tcat_Jan6th_Proud_Boys-20210106-20210106------------fullExport--9654fe3ff4.csv\"\n",
    "twitter_data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates, inplace allows us to overwrite the data in memory\n",
    "twitter_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the first five entries in our data with no duplicates\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove a few columns \n",
    "twitter_data = twitter_data.drop(['withheld_copyright', 'withheld_scope', 'truncated', 'lat', 'lng',\n",
    "                                 'from_user_utcoffset', 'from_user_timezone', 'from_user_lang', \n",
    "                                 'from_user_withheld_scope'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some regular expressions\n",
    "# regular expressions -\n",
    "# lambda: apply the same operation on a given subset of the data, you get to define what that function is\n",
    "#RT @someletters:\n",
    "remove_rt = lambda x: re.sub(\"RT @\\w+: \",\"\",x)\n",
    "rt = lambda x: re.sub('(@[A-Za-z0-9]+)',\" \",x)\n",
    "twitter_data['text'] = twitter_data['text'].map(remove_rt).map(rt)\n",
    "twitter_data['text'] = twitter_data['text'].str.lower()\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Machine learning modelling\n",
    "\n",
    "Natural language processing (sentiment analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a couple new columns: polarity and subjectivity\n",
    "# polarity is a range between (-1, 1). 1 is a positive statement, -1 is a negative statement\n",
    "# subjectivity refers to personal opinion, emotion or judgement, whereas objective refers to fact\n",
    "# subjectivity ranges from (0,1), where 0 is pure personal emotion, 1 is known fact\n",
    "twitter_data[['polarity','subjectivity']] = twitter_data['text'].apply(lambda Text : pd.Series(TextBlob(Text).sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing a score for the text column using SentimentIntensityAnalyzer \n",
    "# If you have a \"lexicon error\", try the following\n",
    "nltk.download('vader_lexicon')\n",
    "for index,row in twitter_data['text'].iteritems():\n",
    "    # compute a score\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    # Assign score categories to variables\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    \n",
    "    # If negative score (neg) is greater than positive score (pos), then the text should be categorized as \"negative\"\n",
    "    if neg> pos:\n",
    "        twitter_data.loc[index,\"sentiment\"] = 'negative'\n",
    "    # If positive score (pos) is greater than the negative score (neg), then the text should be categorized as \"positive\"\n",
    "    elif pos > neg:\n",
    "        twitter_data.loc[index,\"sentiment\"] = \"positive\"\n",
    "    # Otherwise \n",
    "    else:\n",
    "        twitter_data.loc[index,\"sentiment\"] = \"neutral\"\n",
    "        twitter_data.loc[index,'neg'] = neg\n",
    "        twitter_data.loc[index,'pos'] = pos\n",
    "        twitter_data.loc[index,'neu'] = neu\n",
    "        twitter_data.loc[index,'compound'] = comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's go back in to our data set and look specifically at the variable sentiment. What are the unique values?\n",
    "twitter_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data visualizing \n",
    "In this section we want to understand the polarity and subjectivity of the tweets in our sample in a visual format. This will give us the ability to summarize thousands of Tweets in a more meaningful representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at how many are labelled positive, negative or neutral\n",
    "tw_list_negative = twitter_data[twitter_data['sentiment']=='negative']\n",
    "tw_list_positive = twitter_data[twitter_data['sentiment']=='positive']\n",
    "tw_list_neutral = twitter_data[twitter_data['sentiment']=='neutral']\n",
    "\n",
    "# Let's count how many of these values belong to each category. We will define a function to count values.\n",
    "def count_values_in_column(data,feature):\n",
    "    \n",
    "    total = data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage = round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    \n",
    "    return pd.concat([total,percentage],axis=1, keys=['Total', 'Percentage'])\n",
    "\n",
    "# Values for sentiment\n",
    "pc = count_values_in_column(twitter_data, \"sentiment\")\n",
    "\n",
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a piechart\n",
    "names = pc.index\n",
    "size = pc['Percentage']\n",
    "my_circle = plt.Circle((0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names,colors=['blue','red','green'])\n",
    "p = plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.jpeg\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "    mask = mask,\n",
    "    max_words=3000,\n",
    "    stopwords=stopwords,\n",
    "    repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for all tweets\n",
    "create_wordcloud(twitter_data[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for positive sentiment\n",
    "create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for negative sentiment\n",
    "create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet’s length and word count\n",
    "twitter_data['text_len'] = twitter_data['text'].astype(str).apply(len)\n",
    "twitter_data['text_word_count'] = twitter_data['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(twitter_data.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(twitter_data.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0–9]+', '', text)\n",
    "    return text\n",
    "twitter_data['punct'] = twitter_data['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#Applying tokenization- splitting a phrase, sentence, paragraph, or an entire text document into smaller units\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "twitter_data['tokenized'] = twitter_data['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "twitter_data['nonstop'] = twitter_data['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#Applying Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "twitter_data['stemmed'] = twitter_data['nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(twitter_data['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum(),columns=[\"Value\"])\n",
    "countdf = count.sort_values(\"Value\",ascending=False).head(20)\n",
    "\n",
    "px.bar(countdf[1:],x=countdf.index[1:],y=\"Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 Conclusions\n",
    "\n",
    "Refer back to our first table made under section 4 visualization - the majority of tweets (86.89%) of the tweets were labelled as positive, whereas only 12.37% were labelled as negative. We find word clouds that share similar patterns with prominent display of terms like \"proud boys\", \"capitol\" and \"national guard\". The most prominent terms include \"boy\", \"stay\", \"trump\", \"people\". Why are the tweets so overwhelmingly positive?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
